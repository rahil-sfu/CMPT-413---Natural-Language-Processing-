{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunker.py: Option 2 Implementation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from option2_rnn import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1027/1027 [00:02<00:00, 500.06it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 12081 phrases; correct: 9342.\n",
      "accuracy:  87.17%; (non-O)\n",
      "accuracy:  88.22%; precision:  77.33%; recall:  78.53%; FB1:  77.92\n",
      "             ADJP: precision:  51.72%; recall:  19.91%; FB1:  28.75  87\n",
      "             ADVP: precision:  70.77%; recall:  46.23%; FB1:  55.93  260\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  76.38%; recall:  82.51%; FB1:  79.33  6737\n",
      "               PP: precision:  90.87%; recall:  87.26%; FB1:  89.03  2344\n",
      "              PRT: precision:  71.79%; recall:  62.22%; FB1:  66.67  39\n",
      "             SBAR: precision:  81.30%; recall:  42.19%; FB1:  55.56  123\n",
      "               VP: precision:  68.61%; recall:  74.18%; FB1:  71.28  2491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.32803575862926, 78.53059852051109, 77.92467781624056)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 Implementation (Old)\n",
    "\n",
    "Firstly, in the process of doing this assignment, we successfully implemented our Option 1 (baseline) implementation which involved implementing the function char_lvl_representation which makes a character level representation of the work. Here v1 is a one-hot vector for the word's first character. v2 contains the count of each character in the word as well as the index of all the characters in between them. The final character of the word is represented by the one-hot vector v3.\n",
    "makes a character level representation of the work. Here v1 is a one-hot vector for the word's first character. v2 contains the count of each character in the word as well as the index of all the characters in between them. The final character of the word is represented by the one-hot vector v3. The implementation was as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:9: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_16122/2391541655.py:9: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if element is not '[UNK]':\n"
     ]
    }
   ],
   "source": [
    " def char_lvl_representation(sentence):\n",
    "    input_vectors = []\n",
    "    size = 100\n",
    "    for element in sentence:\n",
    "        v1 = torch.zeros(size)\n",
    "        v2 = torch.zeros(size)\n",
    "        v3 = torch.zeros(size)\n",
    "\n",
    "        if element is not '[UNK]':\n",
    "            v1[string.printable.find(element[0])] = 1\n",
    "            v3[string.printable.find(element[-1])] = 1\n",
    "            for char_unq in (list(set(element[1:-1]))):\n",
    "                v2[string.printable.find(char_unq)] = element.count(char_unq)\n",
    "    # append to input vectors\n",
    "        input_vectors.append(torch.cat((v1, v2, v3), 0))\n",
    "    return torch.stack(input_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 Implementation (Current)\n",
    "\n",
    "In order to implement Option 2, building on the existing baseline implementation we created a second RNN that receives the character vector representation as input and produces its hidden state after concatenating it with the word embeddings. This output then goes through the LSTM.\n",
    "In order to be fed into the RNN cell alongside the initial input in the sequence, a hidden state is seeded as a matrix of zeros in the first step. The input data will be multiplied by the hidden state using weight matrices. In order to induce non-linearity, the product of these multiplications will then be sent through an activation function (here we have tanh function). This reveals the RNN cell's hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class secondRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(secondRNN, self).__init__()\n",
    "    #  assigning to relavent variables\n",
    "        self.w_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.w_i = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Tan_h = nn.Tanh()\n",
    "    # forward function\n",
    "    def forward(self, input_sequence, hidden_p):\n",
    "        hid = self.Tan_h(self.w_h(input_sequence) + self.w_i(hidden_p))\n",
    "        return hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "After originally implementing Option 1 (baseline) successfully, our dev.out score increased from 72.70 to 77.21, which is a ~6.20% increase in performance. To further increase the performance of our model, we opted to try to implement the Option 2 improvement which involved developing a second RNN to concatenate the word embeddings and character vectors to add an element of preprocessing of the input to the Chunker RNN. We tested different hidden layer sizes for the RNN, and ultimately we achieved the best dev score of 77.92 with a size of 64, which is a ~7.18% increase in performance over the baseline and a ~0.92% increase in overall performance over the Option 1 implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
