{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensegment: HW 0 Quanteam\n",
    "John Canning, \n",
    "Rahil Sharma, \n",
    "Garvit Sardana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load default.py\n",
    "import re, string, random, glob, operator, heapq, codecs, sys, optparse, os, logging, math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from math import log10\n",
    "\n",
    "#### Code from P. Norvig's book chapter in \"Beautiful Data\"\n",
    "\n",
    "def memo(f):\n",
    "    \"Memoize function f.\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo\n",
    "\n",
    "#### Word Segmentation (p. 223)\n",
    "\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    @memo\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "        candidates = ([first]+self.segment(rem) for first,rem in self.splits(text))\n",
    "        return max(candidates, key=self.Pwords)\n",
    "\n",
    "    def splits(self, text, L=20):\n",
    "        \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "        return [(text[:i+1], text[i+1:]) \n",
    "                for i in range(min(len(text), L))]\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)\n",
    "\n",
    "#### Support functions (p. 224)\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    with open(name) as fh:\n",
    "        for line in fh:\n",
    "            (key, value) = line.split(sep)\n",
    "            yield (key, value)\n",
    "\n",
    "##################################################################################\n",
    "# From the background reading: Natural Language Corpus Data: Beautiful Data\n",
    "# start at a probability of 10./N and decrease bya factor of 10 for every letter \n",
    "# in word.\n",
    "\n",
    "def avoid_long_words(word, N):\n",
    "    return 10./(N * 10**len(word))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-c\", \"--unigramcounts\", dest='counts1w', default=os.path.join('data', 'count_1w.txt'), help=\"unigram counts\")\n",
    "    optparser.add_option(\"-i\", \"--inputfile\", dest=\"input\", default=os.path.join('data', 'input', 'dev.txt'), help=\"file to segment\")\n",
    "    optparser.add_option(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"log file for debugging\")\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    if opts.logfile is not None:\n",
    "        logging.basicConfig(filename=opts.logfile, filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    sys.setrecursionlimit(10**6)\n",
    "    \n",
    "    # the number of tokens the Corpus is |V+| = 1024908267229\n",
    "    # from the background reading: Natural Language Corpus Data: Beautiful Data\n",
    "    Pw = Pdist(data=datafile(opts.counts1w), N=1024908267229, missingfn = avoid_long_words)\n",
    "    segmenter = Segment(Pw)\n",
    "    with open(opts.input) as f:\n",
    "        for line in f:\n",
    "            print(\" \".join(segmenter.segment(line.strip())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "Firstly, I tried continuing with the textbook's implementation of this program, by adding the function avoid_long_words and specifying the size of the Corpus, N = 1024908267229, I was able to improve performance significantly going from the initial dev.out score of 0.82 to 0.98. This is because it lowers the probability the longer the word is by a factor of 10 for each letter in the word. This is an intuitive solution to improve performance because people typically user shorter words than longer words in their vernacular. \n",
    "\n",
    "After this, I tried to follow along further with the textbook and implement a Bigram Model for word segmentation to account for edge cases such as \"sitdown/sit down\" in order to increase the accuracy of the model. Unfortunately, my implementation didn't quite work out and it is likely because I couldn't figure out how I should properly call segment2 instad of segment on line 91. I realized too late that the textbook was using a dataset that was formatted differently for the Bigram implementation and, due to time constraints and being accepted into the course late (27th), I didn't have enough time to see if I could modify the code in order to use a Bigram Model for the counts1_w dataset. Modifying the counts1_w dataset was also out of the question due to the constraints of the assignment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
